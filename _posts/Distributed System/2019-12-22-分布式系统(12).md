---
layout: post
title: 分布式系统(12)
categories: Distributed-System
description: 分布式系统第十二章内容
keywords: 分布式文件系统
---

# 分布式文件系统

## 12.1 简介

本章只讨论基本的分布式文件系统，即在多个远程计算机上模拟非分布式文件系统的功能，不考虑：

- 文件副本冗余
- 多媒体数据流所需的带宽和实时保证

至于对共享对象的永久存储，一种实现方法是序列化对象然后落盘，但是对于经常变化的对象，并不实用。

<img src="2019-12-22-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F(12).assets/image-20191222112652301.png" alt="image-20191222112652301" style="zoom:50%;" />



### 12.1.1 文件系统的特点

文件系统负责文件的组织、存储、检索、命名、共享和保护，文件系统提供描述文件抽象的程序接口

文件包括数据和属性；目录作为特殊的文件，负责其中文件的文本名到内部文件标识符的映射

### 12.1.2 Linux文件系统

### 12.1.3 分布式文件系统的需求

- 透明性
  - 访问
    客户程序不应了解文件的分布性
  - 位置
    客户程序应使用单一的文件命名空间
  - 移动
    文件移动时，客户程序和客户结点上的系统管理表都不必进行修改
  - 性能
    服务负载在一个特定范围内变化时，客户程序性能可以得到满意的性能
  - 伸缩
    文件服务可以不断扩充

- 并发控制

- 文件复制或缓存

- 硬件和操作系统异构性

- 容错：客户和服务器故障了怎么办，通信故障了怎么办。最高级的就是复制（冗余）了。

  为了应对通信故障，可以基于最多一次的语义；或者把文件服务都设置为幂等操作，基于至少一次的语义

  另外，服务器可以设计为**无状态**的。这样有利于错误恢复。

- 一致性：最为麻烦的问题。就是文件如何保持其多个副本的一致状态。比如内存和cache，磁盘和内存，都维护了绝对的一致性，用户无法感受到任何变化。

  但是一旦到了多进程访问同一个文件，就有一致性的取舍。UNIX文件系统只维护唯一的一个拷贝，这意味着所有文件都是一个视图。但是多多少少都有一定的延迟，这又违反了一致性。

- 安全性：访问控制

- 效率：吞吐量、客户延迟等

## 12.2 文件服务体系结构

- 客户模块与服务器通过RPC连接

- 目录服务用于提供全局的文件名<->UFID的解析
- 平面文件服务就是在文件系统中的实际操作

<img src="2019-12-22-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F(12).assets/image-20191222140300639.png" alt="image-20191222140300639" style="zoom:50%;" />

- 平面文件服务接口
  - 与UNIX比较
    - 平面文件服务接口和UNIX的文件系统原语在功能上等价
    - 平面文件服务接口没有open和close操作
    - UNIX中没有平面文件服务接口中的带文件指针的Read和Write操作
  - 容错性差别
      - 可重复操作：除了Create之外，其余操作都是幂等价的
      - 无状态服务器：不需要客户和服务器的任何状态
  
  其实也正是因为分布式文件服务没有open和close，所以其不用在内存中维护和用户的连接，也就成了一个无状态的协议。宕机重启之后仍然可以继续服务。
  
  再加上几乎都是幂等操作，就可以采用至少一次的语义，通信重复故障也可以被屏蔽。
  
  这些设计都是为了分布式文件系统的容错性。
  
- 访问控制：

  - UNIX文件系统在open的时候检查一下访问模式和权限即可。

  - 在分布式文件系统中，访问控制和检查只能在服务器进行实现，否则不安全。用户标识在连接中传输，为了继续保证无连接访问，有两种方案：

    - 每次RPC请求都带着用户标识进行校验
    - 每次文件名转换为UFID时，以权能编码返回给用户，然后之后一系列访问就有权限了。

    至于用户标识的伪造问题，一般以数字签名作为保护机制

- 文件系统的目录组织：完全类比UNIX，包括树形文件系统组织，也包括link操作等

- 文件组：一个位于给定服务器上的文件集合，一个服务器可以包含数个文件组，文件组可以在服务器之间移动。分布式系统中文件组标识必须唯一。

## 12.3 SUN网络文件系统

- NFS中使用的文件标识符成为文件句柄（file handle）

<img src="2019-12-22-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F(12).assets/image-20191222144639569.png" alt="image-20191222144639569" style="zoom:50%;" />

​	i-node号自然不用说，表示在NFS中的i-node号，i-node产生号表示这个i-node号的重用次数

-  VFS层对每一个**<u>打开的文件</u>**给一个v-node，提供一个文件是本地的还是远程的标识。本地的话给出i-node号，远程的话给出一个文件句柄。

  <img src="2019-12-22-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F(12).assets/image-20191222145553611.png" alt="image-20191222145553611" style="zoom:50%;" />

- 客户端模块：

  - 把NFS的客户接口写进UNIX的内核代码，使得直接通过UNIX系统调用就可以访问远程文件，无需动态链接or加载。
  - 客户端模块还可以拥有一个总的共享缓存
  - 用于身份认证的模块同样可以扔到内核内存里，用户级进程也伪造不了

- 访问控制和认证

  - 与传统UNIX文件系统不同，NFS服务器是无状态的
  -  用户每发出一个新的文件请求，服务器必须重新比对用户标识和文件访问许可属性，标识在内核里由RPC协议生成，用户级进程改不动

- 挂载服务：

  - 每个服务器/etc/exports指明一些服务器的本地文件集系统，这些文件集系统可以被用户请求挂载到本地。当然也不是真的都复制过去，利用RPC维护其访问透明性即可。挂载之后用户申请该文件集系统既可以拥有其根目录文件句柄。
  - 硬挂载：请求服务器文件时，会把client用户进程挂起，如果服务器坏了就得等他重启；
  - 软挂载：试了几次之后，就返回错误提示了

- 路径名翻译：由于本地挂载点和服务器的不一样，所以以一种迭代的方式进行路径名翻译，即找到文件的i-node节点引用。多次向服务器提出lookup请求，每一步再进行处理。

- 自动挂载器

  维护一张记录挂载点和对应一个或多个NFS服务器的列表

  - 如果它检测到用户正试图访问一个尚未挂接的文件系统，它就会自动检测该文件系统，如果存在，那么autofs会自动将其挂载

  - 如果它检测到某个已挂接的文件系统在一段时间内没有被使用，那么autofs会自动将其卸载

  - 一旦运行了autofs后，用户就不再需要手动完成文件系统的挂载和卸载

- 服务器缓存：

  - 传统UNIX文件系统里面，主存中的buffer cache专门用来缓存最近访问的磁盘页。并且采用了预测读，延迟写的技术增加吞吐量，提高性能。
  - NFS里面，在读方面多缓存一些没有问题。但是写这里，有两种写操作，一种写透，一种是暂时缓存。用户可以知道server怎么做的。

- 客户缓存：

  类比于UNIX，客户也可以缓存一些请求到的数据资源。为了保证客户手上数据的一致性，NFS的设计就是polling服务器，polling的间隔设为t。

  当客户想要验证当前手上的数据是不是最新的时候，他采取如下操作：

  - 首先，设上一次polling服务器得到验证的时间为t_c，如果t_now-t_c<t，那客户就认为是最新的，否则进入下一条
  - 向服务器发出质询，检查当前数据在服务器的最新更新时间，返回客户得到结果，与本地所知的最新更新时间一样，客户认为是最新的，并且要更新t_c；如果不是最新的，返回false。

  形式化的来讲，就可以有一个有效性条件：
  $$
  (T- T_c <t) ~~~∪ ~~~(T_{mclient}== T_{mserver})
  $$
  分析一下，服务器的每个被打开的文件，t时间段内，总是会被所有客户质询一次。最讨厌的是，T_mserver这个值，并不是针对文件的，而是文件的每个数据块。。。

  有一些方法减少对服务器的请求数：

  - 得到一个T_mserver之后，对所有该文件的派生项都应用之
  - 如果真的变了，将更新结果一起发送
  - t可以由自适应算法来设置

  客户的读写缓存，比如预测读，延迟写，有一个后台进程帮忙异步完成。

- 一致性的问题：

  - t间隔内，一致性肯定难以保证
  - 客户写了，如果不把脏页告诉服务器，连服务器都不知道有改动，其他客户端更没法知道。

- 其他优化：目的就一个，减少通信过程
  - 增大磁盘块粒度到8KB，顺序访问的请求就少了
  - 增大UDP数据包到9KB，请求就少了
  - 所有文件和目录的请求都直接更新T_mserver，质询次数也可以控制

- SUN NFS总结

  - 性能：和本地磁盘相比不低。但是有两个性能瓶颈

    - 大文件写透
    - 3s的质询

    然而事实上写操作不多，占多数的是lookup操作，这是由于路径名翻译的迭代机制产生的

  - 访问透明性：NFS的RPC接口几乎和UNIX原生的一致

  - 位置透明性：本地挂载

  - 移动透明性：客户端的挂载表需要手动更新

  - 复制：只读的可以，带更新的没有设计

  - 异构、伸缩、安全、效率：都不错

  - 容错：由于无状态协议和幂等操作，容错性基本和本地一样，服务器挂了挂起等他好了就可以继续了

  - 一致性：近似于UNIX单拷贝语义，不过还比较弱，如果需要同步的比较紧密的，不建议使用。

## 12.4 Andrew文件系统

NFS的一个缺陷就是伸缩性一般，尤其是3s的质询，在客户越来越多的时候，对文件服务器的压力线性增长，很容易压垮服务器，基于此，Andrew文件系统诞生。

它与NFS有几个主要区别：

- 整体文件服务：它的传输粒度经常是文件，NFS一般是数据块；
- 整体文件缓存：客户机大范围缓存服务器的文件，很多时候客户机是可以独立工作的；
- push型的一致性维护：同NFS一样，有着近似的类UNIX单一拷贝一致性维护；不过与NFS不同的是，AFS不会polling服务器，而是在文件有变动的时候将更新通知给客户端（回调承诺）；
- 不严格的无状态协议：AFS并不是严格的无状态，他要记录每个client打开了什么文件，以便在这些文件更新的时候发一个回调；但是也可以称为无状态的，因为这些信息被放到

### 12.4.1 实现

<img src="2019-12-22-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F(12).assets/image-20191222205805497.png" alt="image-20191222205805497" style="zoom:50%;" />

如图所示，客户机的Venus进程是一个用户进程，它负责对共享文件的管理、与文件服务器的通信，以及维护一个层次目录结构。

- 当用户对共享文件系统中的文件执行open时：
  1. UNIX kernel判断其是本地的还是共享的，如果是共享的，进入Venus进程
  2. 如果Venus判断该文件在本地有缓存，那么进入3；没有的话进入4
  3. Venus验证回调标志，如果为true，直接得到结果，否则进入4
  4. Venus将文件路径名翻译成一个fid（文件卷号+文件句柄+唯一标识），不断向服务器查找相应的fid，找到后缓存回客户端。

- Venus管理共享文件：
  
- 共享空间满的时候，LRU替换算法
  
- Venus管理目录结构：

  <img src="2019-12-22-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F(12).assets/image-20191222212208695.png" alt="image-20191222212208695" style="zoom:47%;" />

  - 如图所示，基本手段就是在一个固定的CMU分区下挂载共享文件系统，其他本地的文件系统link到这边来。大量的UNIX文件都通过link到了共享文件里。这意味着用户在任何一个客户机都可以找到自己的用户文件。

- Vice负责实现一个平面文件服务，实现基本的RPC接口

- 缓存一致性的问题：目的是尽量实现类UNIX的单拷贝语义

  - Vice更新文件的时候，向所有拥有该缓存的文件发一个回调，将它们的回调标志设为false。
  - 客户机更新或重启的时候，所有回调标志设false，要去跟服务器check文件的时间戳(Last modified)，然后重新设置；客户机拿到文件后长时间不跟服务器通信试也是这样。目的是为了避免通信故障。
  - 用户拿到文件之后，随意更新，close的时候，发回文件副本，取代服务器当前版本。准确的说，AFS没有实现控制并发的机制。

### 12.4.2 评估

- UNIX内核可以只为fid转换那里简单修改
- 只读复制：可以提供一个读写拷贝，其他都是读复制
- 批量传输：传文件的粒度非常大
- 部分文件缓存：虽然以文件为粒度缓存，也支持部分文件缓存
- 性能：由于push的回调承诺和文件缓存，省下了大量带宽和服务器负载，让服务器的可扩展性得到了很大提升

AFS和NFS都存在的一个问题就是C-S架构的问题，单点故障，性能瓶颈都集中在核心文件服务器上，这样一台robust的服务器很贵。

## 12.5 GFS

一份文件分成若干个Chunk，3份拷贝，存在chunkserver上，master只存metadata

- Chunk尺寸：
  - 比较有趣，GFS把Chunk尺寸放到64MB这么大，因为它的需求经常是大文件和超大型文件
  - 另外，为了尽量减少master压力，chunk尺寸放大，可以减少和master的通信，转而和slave进行持续的TCP连接；Master存储metadata的存储开销也小了不少
  - 一个缺陷就是小文件，就1个Chunk

<img src="2019-12-22-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F(12).assets/image-20191223151808925.png" alt="image-20191223151808925" style="zoom:67%;" />

- 缓存：GFS的缓存策略很有意思：尽量不缓存
  - 服务器：没有额外的策略，只有Linux原生的buffer cache机制
  - 客户端：除了缓存从master那里得到的文件位置信息外，均不缓存，尤其是文件数据，统统不缓存。
- 服务器故障：
  - 服务器根本不持久化各个Chunk的位置信息，它故障重启之后负责polling各个Chunk Server的信息
  - 各个chunk server通过heartbeat向master汇报节点信息
- 读操作：
  - 向master质询，得到位置信息
  - 向chunk server质询得到数据
- 写操作：
  - 向master质询，得到位置信息，包括master临时指定的一个primary， 其余的为secondary，并向primary指派一个lease（租约）
  - 客户向其中一个写数据，数据在这些服务器之间串行复制，都在缓冲区中
  - primary决定一个串行化顺序，自己写好了之后告诉secondary这个顺序，然后从Secondary那里得到回复
  - 回复齐了，给客户发确认信息
- append操作：
  - 和写操作类似，但是chunk溢出了就要